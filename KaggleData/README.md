# About Dataset
## Context
Distributed micro-services based applications are typically accessed via APIs. These APIs are used either by apps or they can be accessed directly by programmatic means. Many a time API access is abused by attackers trying to exploit the business logic exposed by these APIs. The way normal users access these APIs is different from how the attackers access these APIs. Many applications have 100s of APIs that are called in specific order and depending on various factors such as browser refreshes, session refreshes, network errors, or programmatic access these behaviors are not static and can vary for the same user. API calls in long running sessions form access graphs that need to be analysed in order to discover attack patterns and anomalies. Graphs dont lend themselves to numerical computation. We address this issue and provide a dataset where user access behavior is qualified as numerical features. In addition we provide a dataset where raw API call graphs are provided. Supporting the use of these datasets two notebooks on classification, node embeddings and clustering are also provided.

## About the dataset
There are 4 files provided. Two files are in CSV format and two files are in JSON format. The files in CSV format are user behavior graphs represented as behavior metrics. The JSON files are the actual API call graphs. The two datasets can be joined on a key so that those who want to combine graphs with metrics could do so in novel ways.

### What is new in this dataset
This data set captures API access patterns in terms of behavior metrics. Behaviors are captured by tracking users' API call graphs which are then summarized in terms of metrics. In some sense a categorical sequence of entities has been reduced to numerical metrics.

### CSV dataset
There are two files provided. One called supervised_dataset.csv has behaviors labeled as normal or outlier. The second file called remaining_behavior_ext.csv has a larger number of samples that are not labeled but has additional insights as well as a classification created by another algorithm.

### What is each row
Each row is one instance of an observed behavior that has been manually classified as normal or outlier

### JSON dataset
There are two files provided to correspond to the two CSV files

### What is each item
Each item has an _id field that can be used to join against the CSV data sets. Then we have the API behavior graph represented as a list of edges.

### Inspiration
To model the classification label with a skewed distribution of normal and abnormal cases and with very few labeled samples available. Use supervised_dataset.csv
To verify where the predicted class differs from the class determined by a second algorithm. Use remaining_behavior_ext.csv

## Some more info from a Kaggle discussion
Question:
"Each of these graphs is a real API call graph interaction captured from production environment of two diverse distributed microservices-driven applications."
Now when you captured these call graphs, exactly what procedure did you follow to label these graphs as normal/outlier ?
Answer:
Microservices were instrumented with language agents. As the APIs are called, API calls are grouped into into sequences based on the session-id passing through different microservices. These sequences of API calls were represented as graphs. For example if you have A->B -> C -> A -> D as the sequence, the graph will have the following edges [A->B, A -> D, B->C, C -> A]. Every graph is assigned an UUID. The _id is the UUID for the call graph. The graph itself does not have a class label assigned.

What we need to do is use the UUID to look up in the dataset called supervised_dataset.csv. So the _id field in the file called supervised_dataset.csv corresponds to the _id in supervised_call_graph.json. Think of joining the 2 datasets on the _id field. Now, supervised_dataset.csv has a class label which is the column classification

How was the label generated? This label was generated by an anomaly detector whose result was manually vetted based on a multi-step decision tree (not a trained model decision tree). Some of the decision rules were as follows:

did the anomaly detector detect that user's API call graph as Normal. If it was Normal then did this user fall unto a cluster where there are many more users with the same call graph (high density of users). If it is high-density cluster then it is very likely to be normal call graph. hence the label is normal.
did the anomaly detector detect that the user's API call graph as Outlier. If it was an Outlier then were users accessing from multiple Geo-locations not including the USA. We were expecting the APIs to be accessed only from the USA as there was no reason for people from outside the USA to use these APIs. The business was not for users outside the USA. The fact that the anomaly detector called out the API call graph as an outlier and we have this "outside USA access situation" the outlier becomes a true outlier and the label becomes 'Outlier'
There are a few other rules that are applied and then manually vetted to assign class labels.
cases where we were not able to manually ascertain the true class label were grouped into the remaining_behaviour_ext.csv dataset